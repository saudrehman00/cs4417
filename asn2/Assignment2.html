<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="dcterms.date" content="2023-02-23" />
  <title>Unstructured Data - Assignment 2</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Unstructured Data - Assignment 2</h1>
<p class="date">23 February 2023</p>
</header>
<h1 id="assignment-2">Assignment 2</h1>
<p>In this assignment, you will use various transformer models for
semantic search and for language generation. We will be using the
<code>transformers</code> python package from huggingface;
<strong>note</strong> that this package will automatically download
language models as required the first time the code is run, and they can
be quite large. (The entire assignment might download a few GB.) You
might want to do this on campus, depending on your internet
situation.</p>
<p>This assignment is to be done individually. You may discuss the
project with your classmates, but the work you turn in should be your
own.</p>
<h1 id="part-1---comparing-and-using-embeddings">Part 1 - Comparing and
Using Embeddings</h1>
<h2 id="goal">Goal</h2>
<p>The main goal of this part of the assignment is to experiment with
different embedding techniques in an information retrieval context. It
also reinforces the definition and use of the <code>json</code>
format.</p>
<h2 id="setup">Setup</h2>
<p>This assignment will use python. We will use the
<code>sentence-transformers</code> python package and its
dependencies.</p>
<p><a href="https://huggingface.co/sentence-transformers"
class="uri">https://huggingface.co/sentence-transformers</a></p>
<p>If you have python installed on your machine, you can use</p>
<pre><code>pip3 install -U sentence-transformers</code></pre>
<p>If you do not have python, you can install conda from <a
href="https://anaconda.org/" class="uri">https://anaconda.org/</a> and
then install sentence-transformers with</p>
<pre><code>conda install -c conda-forge sentence-transformers</code></pre>
<p>If you are already familiar with python and conda environments, you
can work however you wish.</p>
<p>Use the provided <code>A2Part1.py</code> file as a template, and the
attached file <code>tweets-utf-8.json.zip</code>, which contains tweets
geolocated to London and Ottawa over a period of time in 2017. Unzip
this prior to starting.</p>
<h2 id="questions">Questions</h2>
<h3 id="coding-40-pts">Coding (40 pts)</h3>
<ol type="1">
<li><p>Write a function <code>get_tweets()</code> that uses the
<code>json</code> python package to read the tweets from
<code>tweets-utf-8.json</code> and produces a list of strings that
contain the text of each tweet. (Each line of
<code>tweets-utf-8.json</code> contains one json object.)</p></li>
<li><p>Write a function
<code>sort_by_sim(query_embedding,document_embeddings,documents)</code>
that takes the embedding of a query document, a list of document
embeddings, and a list of the corresponding documents, and returns a
list of pairs of the form <code>(similarity,document)</code>, sorted in
decreasing order according to cosine similarity between each document
and the query. You can use any packages you like; note that
<code>numpy</code> has a <code>dot</code> function. If a similarity
computation would involve a divide by zero, define the similarity to be
<code>0</code> instead (This is not correct, but is OK for our
purposes.)</p></li>
<li><p>Write a function top25_glove() that returns the top 25 most
similar tweets (as (similarity,document) pairs) to the query “I am
looking for a job.” using the glove-based sentence embedding defined
here: <a
href="https://huggingface.co/sentence-transformers/average_word_embeddings_glove.840B.300d"
class="uri">https://huggingface.co/sentence-transformers/average_word_embeddings_glove.840B.300d</a></p></li>
<li><p>Write a function top25_minilm() that returns the top 25 most
similar tweets (as (similarity,document) pairs) to the query “I am
looking for a job.” using the MiniLM-based (derived from BERT) sentence
embedding defined here: <a
href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"
class="uri">https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</a>
<strong>FYI</strong> - this model takes quite a bit longer to run
(almost 10 minutes on my laptop).</p></li>
</ol>
<h3 id="intepreting-10-pts">Intepreting (10 pts)</h3>
<p>Answer the following questions in a file called
<code>A2Part1.txt</code>.</p>
<p>Examine the output from both models given our test query, “I am
looking for a job.”</p>
<ol type="1">
<li><p>Identify two differences in the overall results of the two
methods and explain why these differences might be occurring.</p></li>
<li><p>Try out the query in Twitter’s own search on their website. (Note
you don’t need an account to try it.) Do you think Twitter might be
using a semantic search technique like the ones you tried? Why or why
not?</p></li>
</ol>
<h2 id="part-1-deliverables">Part 1 Deliverables</h2>
<p>Submit your <code>A2Part1.py</code> and <code>A2Part1.txt</code>
files as an attachment on OWL. <strong>Submissions will only be accepted
through OWL.</strong></p>
<hr />
<h1 id="part-2---using-generative-language-models">Part 2 - Using
Generative Language Models</h1>
<h2 id="goal-1">Goal</h2>
<p>To learn about how generative language models can be used in
practice, focusing on GPT-2.</p>
<h2 id="setup-1">Setup</h2>
<p>This part uses the <code>transformers</code> package which can be
installed with conda or pip.</p>
<h2 id="questions-25-pts">Questions (25 pts)</h2>
<ol type="1">
<li>Write a script that generates a “story” using a local GPT-2 model.
Your story should: 1) be at least 100 words long; 2) not have repeated
phrases; and 3) be the same every time your script is run. It might be
nonsensical and/or hilarious. Use the skeleton code provided in
<code>A2Part2.py</code> as a starting point, and <a
href="https://huggingface.co/blog/how-to-generate"
class="uri">https://huggingface.co/blog/how-to-generate</a> as a
reference document. Record your story in a file called
<code>A2Part2.txt</code>.</li>
</ol>
<p>Note that the provided <code>A2Part2.py</code> uses pytorch rather
than the older TensorFlow which is used in the reference document. The
syntax is pretty much identical except for the setup, which we provided.
If you really want to use TensorFlow instead, that’s fine too.</p>
<h2 id="part-2-deliverables">Part 2 Deliverables</h2>
<p>Submit your <code>A2Part2.py</code> as an attachment on OWL along
with your story in a file <code>A2Part2.txt</code>. <strong>Submissions
will only be accepted through OWL.</strong></p>
<hr />
<h1 id="checklist">Checklist</h1>
<p>Your owl submission should include the following attachments and no
additional files:</p>
<pre><code>A2Part1.py
A2Part1.txt
A2Part2.py
A2Part2.txt</code></pre>
</body>
</html>
